# Unique identifier for the cluster
cluster_name: whisper-serve-cluster

provider:
    type: aws
    region: us-west-2
    availability_zone: us-west-2a
    cache_stopped_nodes: False
    key_pair:
        key_name: ray-cluster

auth:
    ssh_user: ubuntu
    ssh_private_key: ~/.ssh/ray-cluster.pem

# Global max_workers to prevent autoscaler conflict
max_workers: 3  # Matches sum of min_workers below

available_node_types:
    head_node:
        resources:
            CPU: 8
        node_config:
            InstanceType: t3.2xlarge
            ImageId: ami-05c1fa8c9881244b6
            SubnetId: subnet-c00ae9a6
            SecurityGroupIds: ["sg-030176ebd4455dcc5"]
            KeyName: ray-cluster
            IamInstanceProfile:
                Arn: arn:aws:iam::892335585962:instance-profile/ray-autoscaler-v1
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 100

    gpu_worker:
        resources:
            CPU: 8
            GPU: 1
        node_config:
            InstanceType: g5.4xlarge
            ImageId: ami-05c1fa8c9881244b6
            SubnetId: subnet-c00ae9a6
            SecurityGroupIds: ["sg-030176ebd4455dcc5"]
            KeyName: ray-cluster
            IamInstanceProfile:
                Arn: arn:aws:iam::892335585962:instance-profile/ray-autoscaler-v1
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 100
        min_workers: 1
        max_workers: 1

    cpu_worker:
        resources:
            CPU: 8
        node_config:
            InstanceType: c5.4xlarge
            ImageId: ami-05c1fa8c9881244b6
            SubnetId: subnet-c00ae9a6
            SecurityGroupIds: ["sg-030176ebd4455dcc5"]
            KeyName: ray-cluster
            IamInstanceProfile:
                Arn: arn:aws:iam::892335585962:instance-profile/ray-autoscaler-v1
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 100
        min_workers: 2
        max_workers: 2

head_node_type: head_node

file_mounts:
    "/home/ubuntu/mount_files": "/home/marco/Development/mglabs/stt-server/mount_files"
    "/home/ubuntu/.ssh/ray-cluster.pem": "~/.ssh/ray-cluster.pem"

setup_commands:
    - sudo apt-get update
    - sudo apt-get install -y software-properties-common
    - sudo apt-get install -y git binutils rustc cargo pkg-config libssl-dev
    - pip install --upgrade pip setuptools packaging
    - pip install -U "ray[default,serve]" openai-whisper fastapi python-multipart soundfile
    - pip uninstall -y torch torchvision torchaudio
    - pip install --pre torch  --index-url https://download.pytorch.org/whl/cpu

worker_setup_commands:
    - if lspci | grep -i nvidia; then
        echo "GPU detected. Installing GPU-compatible PyTorch.";
        pip uninstall -y torch torchvision torchaudio;
        pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121;
      fi

head_setup_commands:
    - git clone https://github.com/aws/efs-utils.git /tmp/efs-utils
    - cd /tmp/efs-utils && ./build-deb.sh
    - cd /tmp/efs-utils/build && sudo apt-get install -y ./amazon-efs-utils-*.deb
    - sudo mkdir -p /shared/models
    - sudo mkdir -p /shared/logs
    - echo "fs-08e6f8d79aaa5fd6e.efs.us-west-2.amazonaws.com:/ /shared efs _netdev,tls,iam 0 0" | sudo tee -a /etc/fstab
    - sudo mount -a
    - sudo chown -R ubuntu:ubuntu /shared
    - sudo chown ubuntu:ubuntu /home/ubuntu/.ssh/ray-cluster.pem
    - sudo chmod 400 /home/ubuntu/.ssh/ray-cluster.pem
    - pip install /home/ubuntu/mount_files/whisper_serve-0.1.tar.gz
    - cp /home/ubuntu/mount_files/config.json /home/ubuntu/config.json

head_start_ray_commands:
    - ray stop
    - export RAY_NODE_MANAGER_HEARTBEAT_TIMEOUT_MILLISECONDS=60000
    - export RAY_GCS_RPC_SERVER_REQUEST_TIMEOUT_MS=30000
    - export RAY_OBJECT_MANAGER_PULL_TIMEOUT_MS=30000
    - export RAY_CLIENT_MODE=0
    - export RAY_DASHBOARD_PORT=8265
    - export RAY_DASHBOARD_HOST=0.0.0.0
    - ulimit -n 65536; ray start --head --port=6379 --object-manager-port=8076 --dashboard-host=0.0.0.0 --include-dashboard=true --ray-client-server-port=10001 --disable-usage-stats

worker_start_ray_commands:
    - ray stop
    - export RAY_NODE_MANAGER_HEARTBEAT_TIMEOUT_MILLISECONDS=60000
    - export RAY_GCS_RPC_SERVER_REQUEST_TIMEOUT_MS=30000
    - export RAY_OBJECT_MANAGER_PULL_TIMEOUT_MS=30000
    - ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076 --disable-usage-stats
