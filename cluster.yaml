# An unique identifier for the head node and workers of this cluster.
cluster_name: whisper-serve-cluster

# The maximum number of workers nodes to launch in addition to the head
# node.
max_workers: 8

# The autoscaler will scale up the cluster faster with higher upscaling speed.
# E.g., if the task requires adding more nodes then autoscaler will gradually
# scale up the cluster in chunks of upscaling_speed*currently_running_nodes.
# This number should be > 0.
upscaling_speed: 1.0

# If a node is idle for this many minutes, it will be removed.
idle_timeout_minutes: 5

# AWS details
provider:
    type: aws
    region: us-west-2
    availability_zone: us-west-2a
    cache_stopped_nodes: False
    key_pair:
        key_name: ray-cluster

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: ubuntu
    ssh_private_key: ~/.ssh/ray-cluster.pem

# Tell the autoscaler the allowed node types and the resources they provide.
# The key is the name of the node type, which is just for debugging purposes.
# The node config specifies the launch config and physical instance type.
available_node_types:
    head_node:
        resources:
            CPU: 8
        node_config:
            InstanceType: c5.2xlarge
            ImageId: ami-05c1fa8c9881244b6
            SubnetId: subnet-c00ae9a6
            SecurityGroupIds: ["sg-030176ebd4455dcc5"]
            KeyName: ray-cluster
            IamInstanceProfile:
                Arn: arn:aws:iam::892335585962:instance-profile/ray-autoscaler-v1
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 100

    gpu_worker:
        resources:
            CPU: 8
            GPU: 1
        node_config:
            InstanceType: g5.2xlarge
            ImageId: ami-05c1fa8c9881244b6
            SubnetId: subnet-c00ae9a6
            SecurityGroupIds: ["sg-030176ebd4455dcc5"]
            KeyName: ray-cluster
            IamInstanceProfile:
                Arn: arn:aws:iam::892335585962:instance-profile/ray-autoscaler-v1
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 100
        min_workers: 1
        max_workers: 8

    cpu_worker:
        resources:
            CPU: 8
        node_config:
            InstanceType: c5.4xlarge
            ImageId: ami-05c1fa8c9881244b6
            SubnetId: subnet-c00ae9a6
            SecurityGroupIds: ["sg-030176ebd4455dcc5"]
            KeyName: ray-cluster
            IamInstanceProfile:
                Arn: arn:aws:iam::892335585962:instance-profile/ray-autoscaler-v1
            BlockDeviceMappings:
                - DeviceName: /dev/sda1
                  Ebs:
                      VolumeSize: 100
        min_workers: 0
        max_workers: 0

# Specify the node type of the head node (as configured above).
head_node_type: head_node

# Files or directories to copy to the head and worker nodes. Th
file_mounts:
#    "/home/ubuntu/mount_files": "/home/marco/Development/mglabs/stt-server/mount_files"
    "/home/ubuntu/.ssh/ray-cluster.pem": "~/.ssh/ray-cluster.pem"

# List of shell commands to run to set up nodes.
#
# 1. Setup commands for all nodes.
setup_commands:
    - >
      while sudo lsof /var/lib/dpkg/lock >/dev/null 2>&1 || 
            sudo lsof /var/lib/apt/lists/lock >/dev/null 2>&1 || 
            sudo lsof /var/lib/dpkg/lock-frontend >/dev/null 2>&1 || 
            sudo lsof /var/cache/apt/archives/lock >/dev/null 2>&1; do
        echo "Waiting for other package managers to finish...";
        sleep 5;
      done
    - sudo systemctl stop unattended-upgrades
    - sudo killall apt-get >/dev/null 2>&1 || true
    - sudo apt-get update
    - sudo apt-get install -y ffmpeg
#    - sudo apt-get install -y software-properties-common
#    - sudo apt-get install -y git binutils rustc cargo pkg-config libssl-dev 
#    - git clone https://github.com/aws/efs-utils.git /tmp/efs-utils
#    - cd /tmp/efs-utils && ./build-deb.sh
#    - cd /tmp/efs-utils/build && sudo apt-get install -y ./amazon-efs-utils-*.deb
    - pip install --upgrade pip setuptools packaging
    - pip install -U "ray[default,serve]"
    - pip install -U openai-whisper fastapi python-multipart soundfile
#    - pip uninstall -y torch torchvision torchaudio
#    - pip install --pre torch --index-url https://download.pytorch.org/whl/cpu
#    - pip install transformers
    - sudo mkdir -p /home/ubuntu/logs
    - sudo chmod 755 /home/ubuntu/logs
    - sudo chown -R ubuntu:ubuntu logs
#    - sudo systemctl enable amazon-efs-mount-watchdog
#    - sudo systemctl start amazon-efs-mount-watchdog

# 2. Workers additional setup commands
worker_setup_commands:
    - while sudo lsof /var/lib/dpkg/lock-frontend >/dev/null 2>&1; do echo "Waiting for dpkg lock..."; sleep 5; done
    - echo "Setup starting" > /tmp/worker_setup.log
    - env | grep RAY_ >> /tmp/worker_setup.log
#    - if lspci | grep -i nvidia; then
#        echo "GPU detected. Installing GPU-compatible PyTorch.";
#        pip uninstall -y torch torchvision torchaudio;
#        pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121;
#      fi
    # workers are all GPU - install cuda version
    # pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121

    - pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121;
    - pip install transformers
    - sudo mkdir -p /shared/models 
    - sudo mkdir -p /shared/logs 
#    - echo "fs-08e6f8d79aaa5fd6e.efs.us-west-2.amazonaws.com:/ /shared efs _netdev,tls,iam,verify=0 0 0" | sudo tee -a /etc/fstab
#    - sudo mount -a
    - sudo chown -R ubuntu:ubuntu /shared
#    - touch /shared/logs/worker-setup.log  # Test write access
 #   - pip install /home/ubuntu/mount_files/whisper_serve-0.1.tar.gz
 #   - cp /home/ubuntu/mount_files/config.json /home/ubuntu/config.json

# 3. Head additional setup commands
head_setup_commands:
    - >
      while sudo lsof /var/lib/dpkg/lock >/dev/null 2>&1 || 
           sudo lsof /var/lib/apt/lists/lock >/dev/null 2>&1 || 
            sudo lsof /var/lib/dpkg/lock-frontend >/dev/null 2>&1 || 
            sudo lsof /var/cache/apt/archives/lock >/dev/null 2>&1; do
        echo "Waiting for other package managers to finish...";
        sleep 5;
      done
    - sudo systemctl stop unattended-upgrades
    - pip uninstall -y torch torchvision torchaudio
    - pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
    - pip install transformers
#    - sudo mkdir -p /shared/models /shared/logs
#    - echo "fs-08e6f8d79aaa5fd6e.efs.us-west-2.amazonaws.com:/ /shared efs _netdev,tls,iam,verify=0 0 0" | sudo tee -a /etc/fstab
#    - sudo mount -a
#    - sudo chown -R ubuntu:ubuntu /shared
#    - sudo chmod 775 /shared/logs
#    - touch /shared/logs/head-setup.log  
    - sudo chown ubuntu:ubuntu /home/ubuntu/.ssh/ray-cluster.pem
    - sudo chmod 400 /home/ubuntu/.ssh/ray-cluster.pem
#    - pip install /home/ubuntu/mount_files/whisper_serve-0.1.tar.gz
#    - cp /home/ubuntu/mount_files/config.json /home/ubuntu/config.json

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
    - ray stop
    - >
      export RAY_NODE_MANAGER_HEARTBEAT_TIMEOUT_MILLISECONDS=600000 &&
      export RAY_HEALTH_CHECK_TIMEOUT_MS=300000 &&
      export RAY_HEARTBEAT_TIMEOUT_MILLISECONDS=300000 &&
      export RAY_MAX_MISSED_HEARTBEATS_BEFORE_FAILURE=20 &&
      export RAY_GCS_RPC_SERVER_REQUEST_TIMEOUT_MS=60000 &&
      export RAY_GCS_SERVER_REQUEST_TIMEOUT_SECONDS=120 &&    
      export RAY_GCS_RECONNECT_GRACE_PERIOD_S=60 &&     
      export RAY_GCS_CHECK_FAILURE_LIMIT=10 && 
      export RAY_RAYLET_HEALTH_CHECK_TIMEOUT_SECONDS=120 &&
      export RAY_RAYLET_HEALTH_CHECK_PERIOD_SECONDS=300 &&    
      export RAY_OBJECT_MANAGER_PULL_TIMEOUT_MS=60000 &&
      export RAY_DEBUG_AUTOSCALER=1 &&
      export RAY_CLIENT_MODE=0 &&
      export RAY_DASHBOARD_PORT=8265 &&
      export RAY_DASHBOARD_HOST=0.0.0.0 &&
      ulimit -n 65536 && ray start --head --port=6379 --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host=0.0.0.0 --include-dashboard=true --ray-client-server-port=10001 --disable-usage-stats

# Command to start ray on worker nodes.
worker_start_ray_commands:
    - ray stop
    - >
      export RAY_NODE_MANAGER_HEARTBEAT_TIMEOUT_MILLISECONDS=600000 &&
      export RAY_HEALTH_CHECK_TIMEOUT_MS=300000 &&
      export RAY_HEARTBEAT_TIMEOUT_MILLISECONDS=300000 &&
      export RAY_MAX_MISSED_HEARTBEATS_BEFORE_FAILURE=20 &&
      export RAY_GCS_RPC_SERVER_REQUEST_TIMEOUT_MS=60000 &&
      export RAY_GCS_SERVER_REQUEST_TIMEOUT_SECONDS=120 && 
      export RAY_GCS_CHECK_FAILURE_LIMIT=10 && 
      export RAY_GCS_RECONNECT_GRACE_PERIOD_S=60 &&
      export RAY_RAYLET_HEALTH_CHECK_TIMEOUT_SECONDS=120 && 
      export RAY_RAYLET_HEALTH_CHECK_PERIOD_SECONDS=300 && 
      export RAY_OBJECT_MANAGER_PULL_TIMEOUT_MS=60000 &&
      export RAY_DEBUG_AUTOSCALER=1 &&
      export RAY_CLIENT_MODE=0 &&
      env | grep RAY_ > /tmp/ray_env.log &&
      ulimit -n 65536 && ray start --address=$RAY_HEAD_IP:6379 --disable-usage-stats