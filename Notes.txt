
---------------------------------------------------------------

---

# Start the cluster in AWS:
ray up cluster.yaml --no-config-cache 

# After the head node is setup -- 

#  To monitor autoscaling:
ray exec /home/marco/Development/mglabs/stt-server/cluster.yaml 'tail -n 100 -f /tmp/ray/session_latest/logs/monitor*'

export RAY_HEAD_ADDRESS=$(ray get-head-ip cluster.yaml | tail -n 1); echo $RAY_HEAD_ADDRESS 
export RAY_DASHBOARD_ADDRESS="http://$RAY_HEAD_ADDRESS:8265"; echo $RAY_DASHBOARD_ADDRESS

# Access the dashboard in a browser at http://$RAY_HEAD_ADDRESS:8265

# After the workers are setup --

# Set up the workload for remote deployment:
cd whisper_serve
zip -r whisper_serve.zip *.py
aws s3 cp whisper_serve.zip s3://ntoplabs-0001

# Start the worload:
serve deploy serve_config.yaml -a $RAY_DASHBOARD_ADDRESS
serve status -a $RAY_DASHBOARD_ADDRESS

# Test endpoint:
curl -X POST http://$RAY_HEAD_ADDRESS:8000/transcribe \
  -F "audio_file=@./audio/test_audio.wav"

# Terminate the cluster
ray down cluster.yaml 

# Log into the head node
ray attach cluster.yaml

-- LOCAL -------------------------------------------------------------

applications:
- name: whisper_service
  route_prefix: /
  import_path: main:app
---

# Start a local cluster:
ray start --head

export RAY_DASHBOARD_ADDRESS="http://127.0.0.1:8265"
# suggest but possibly not really needed
export PYTHONPATH="/home/marco/Development/mglabs/stt-server"

# entry point main.py
serve deploy serve_config.local.yaml -a $RAY_DASHBOARD_ADDRESS 
 
serve status -a $RAY_DASHBOARD_ADDRESS

curl -X POST http://localhost:8000/transcribe \
  -F "audio_file=@./audio/test_audio.wav"

-------------------------------------------------------------
# another working setup using transformers and entry point main-temp1.py

config file: serve_config.local.yaml:
applications:
- name: whisper_service
  route_prefix: /
  import_path: main-temp1:app
...
...
--

export PYTHONPATH="/home/marco/Development/mglabs/stt-server"
serve deploy serve_config.local.yaml -a $RAY_DASHBOARD_ADDRESS 
serve status -a $RAY_DASHBOARD_ADDRESS

serve shutdown -a $RAY_DASHBOARD_ADDRESS
-------------------------------------------------------------

# Watch number of concurrent actors and memory
watch -n 1 'nvidia-smi | grep "WhisperTranscriber" | wc -l'
nvidia-smi -l 1

# Monitor setup (after head node started)
ray exec cluster.yaml 'tail -n 100 -f /tmp/ray/session_latest/logs/monitor*'

-------------------------------------------------------------


# Get head node IP
ray get-head-ip cluster.yaml
export RAY_HEAD_ADDRESS=$(ray get-head-ip cluster.yaml | tail -n 1); echo $RAY_HEAD_ADDRESS

# Check cluster status
ray status --address=$RAY_HEAD_ADDRESS:6379

# Test service
curl -X POST http://$RAY_HEAD_ADDRESS:8000/transcribe \
  -F "audio_file=@./audio/test_audio.wav"

# Check Ray dashboard
curl http://$RAY_HEAD_ADDRESS:8265

# Log into the head node (ssh)
ray attach cluster.yaml

# Find workers IP
aws ec2 describe-instances --filters "Name=tag:ray-node-type,Values=worker" "Name=tag:ray-cluster-name,Values=whisper-serve-cluster" --query 'Reservations[*].Instances[*].PublicIpAddress' --output text


# Sequence of checks to perform after the head node is started:

# Check Ray Dashboard accessibility:
curl http://<head-node-ip>:8265/api/overview

Check Ray Client port:
nc -vz <head-node-ip> 10001

# Check GCS port:
nc -vz <head-node-ip> 6379

# Check Node logs:
```bash
ray exec cluster.yaml 'tail -n 50 /tmp/ray/session_*/logs/*'

# Check Ray status:
ray status cluster.yaml

# Verify environment variables:
ray exec cluster.yaml 'env | grep RAY_'

# Check resource availability:
ray exec cluster.yaml 'ray status --address=localhost:6379 | grep "resources"'
ray exec custer.yaml 'env | grep RAY_'



----------------------------------------------
----------------------------------------------

Starting load test...
Concurrency Avg Latency  P95 Latency  Error Rate
--------------------------------------------
4          0.49........ 0.00........ 0.0.......%
8          0.69........ 0.00........ 0.0.......%
16         1.28........ 0.00........ 0.0.......%
32         2.38........ 4.18........ 0.0.......%
64         4.52........ 8.41........ 0.0.......%
128        9.02........ 17.41....... 0.0.......%
192        14.18....... 25.94....... 0.0.......%
256        18.55....... 36.21....... 0.0.......%
512        37.41....... 71.16....... 0.0.......%


applications:
- name: whisper_service
  route_prefix: /
  import_path: main:app
  runtime_env:
    pip:
      - torch
      - transformers
      - fastapi
      - python-multipart
  deployments:
  - name: WhisperTranscriber
    max_outgoing_requests: 5
    autoscaling_config:
      target_outgoing_requests: 2
      min_replicas: 1
      max_replicas: 50
    ray_actor_options:
      num_cpus: 1
      num_gpus: 0.1
  - name: WhisperAPI
    max_outgoing_requests: 5
    autoscaling_config:
      target_outgoing_requests: 2
      min_replicas: 1
      max_replicas: 50
    ray_actor_options:
      num_cpus: 1


----------------------------------------------
----------------------------------------------

Starting load test...
Concurrency Avg Latency  P95 Latency  Error Rate
--------------------------------------------
4          0.39........ 0.00........ 0.0.......%
8          0.62........ 0.00........ 0.0.......%
16         1.09........ 0.00........ 0.0.......%
32         2.03........ 3.60........ 0.0.......%
64         3.92........ 7.36........ 0.0.......%
128        7.28........ 12.68....... 0.0.......%
192        7.49........ 14.35....... 0.0.......%
256        10.23....... 18.67....... 0.0.......%
512        19.80....... 37.21....... 0.0.......%


applications:
- name: whisper_service
  route_prefix: /
  import_path: main:app
  runtime_env:
    pip:
      - torch
      - transformers
      - fastapi
      - python-multipart
  deployments:
  - name: WhisperTranscriber
    max_outgoing_requests: 10  # Increased from 5
    autoscaling_config:
      target_outgoing_requests: 4  # Increased from 2
      min_replicas: 2  # Increased from 1
      max_replicas: 10  # Reduced from 50 to be more realistic with GPU memory
      upscale_delay_s: 10  # Added to make scaling more responsive
      downscale_delay_s: 30  # Added to prevent rapid scale-down
    ray_actor_options:
      num_cpus: 1
      num_gpus: 0.2  # Increased from 0.1 to allow fewer but more powerful actors
      memory: 4000_000_000  # 4GB memory per actor
  - name: WhisperAPI
    max_outgoing_requests: 15  # Increased from 5
    autoscaling_config:
      target_outgoing_requests: 5  # Increased from 2
      min_replicas: 2  # Increased from 1
      max_replicas: 20  # Adjusted based on CPU availability
    ray_actor_options:
      num_cpus: 1
      memory: 2000_000_000  # 2GB memory per actor


----------------------------------------------
----------------------------------------------

Starting load test...
Concurrency Avg Latency  P95 Latency  Error Rate
--------------------------------------------
4          0.50........ 0.00........ 0.0.......%
8          0.53........ 0.00........ 0.0.......%
16         0.81........ 0.00........ 0.0.......%
32         1.76........ 2.29........ 0.0.......%
64         3.01........ 4.67........ 0.0.......%
128        5.40........ 8.95........ 0.0.......%
192        7.93........ 13.19....... 0.0.......%
256        9.61........ 17.11....... 0.0.......%
512        18.44....... 33.66....... 0.0.......%
proxy_location: EveryNode

http_options:
  host: 0.0.0.0
  port: 8000

applications:
- name: whisper_service
  route_prefix: /
  import_path: main:app
  runtime_env:
    pip:
      - torch
      - transformers
      - fastapi
      - python-multipart
  deployments:
  - name: WhisperTranscriber
    max_outgoing_requests: 25  # Increased for more concurrent processing
    autoscaling_config:
      target_outgoing_requests: 10
      min_replicas: 8  # Increased based on memory availability
      max_replicas: 25  # (16376MiB - 95MiB Xorg) / 592MiB â‰ˆ 27 possible actors
      upscale_delay_s: 10
      downscale_delay_s: 30
    ray_actor_options:
      num_cpus: 1
      num_gpus: 0.037  # 1/27 of GPU (allows maximum concurrent actors)
      memory: 2000_000_000  # Adjusted based on observed usage
      runtime_env:
        env_vars:
          CUDA_VISIBLE_DEVICES: "0"
  - name: WhisperAPI
    max_outgoing_requests: 30
    autoscaling_config:
      target_outgoing_requests: 12
      min_replicas: 8
      max_replicas: 30
    ray_actor_options:
      num_cpus: 1
      memory: 2000_000_000


----------------------------------------------
----------------------------------------------


Concurrency Avg Latency  P95 Latency  Error Rate
--------------------------------------------
4          0.50........ 0.00........ 0.0.......%
8          0.52........ 0.00........ 0.0.......%
16         0.84........ 0.00........ 0.0.......%
32         1.76........ 2.33........ 0.0.......%
64         2.99........ 4.54........ 0.0.......%
128        5.37........ 8.92........ 0.0.......%
192        7.60........ 12.55....... 0.0.......%
256        9.52........ 16.80....... 0.0.......%
512        17.89....... 33.23....... 0.0.......%

proxy_location: EveryNode

http_options:
  host: 0.0.0.0
  port: 8000
  
applications:
- name: whisper_service
  route_prefix: /
  import_path: main:app
  runtime_env:
    pip:
      - torch
      - transformers
      - fastapi
      - python-multipart
  deployments:
  - name: WhisperTranscriber
    max_outgoing_requests: 25
    autoscaling_config:
      target_outgoing_requests: 10
      min_replicas: 8
      max_replicas: 25
      upscale_delay_s: 10
      downscale_delay_s: 30
      upscale_batch_size: 2  # Only addition: control scaling batch size
    ray_actor_options:
      num_cpus: 1
      num_gpus: 0.037
      memory: 2000_000_000
      runtime_env:
        env_vars:
          CUDA_VISIBLE_DEVICES: "0"
  - name: WhisperAPI
    max_outgoing_requests: 30
    autoscaling_config:
      target_outgoing_requests: 12
      min_replicas: 8
      max_replicas: 30
    ray_actor_options:
      num_cpus: 1
      memory: 2000_000_000

WARNING 2024-11-24 13:29:46,412 whisper_service_WhisperTranscriber zyx1jvph replica.py:571 - Replica at capacity of max_ongoing_requests=5, rejecting request 22370ef1-eddb-4c93-a788-324012c9f83f.


=============

cluster 8 workers

  deployments:
  - name: WhisperTranscriber
    max_outgoing_requests: 1
    autoscaling_config:
      target_outgoing_requests: 25
      min_replicas: 8
      max_replicas: 60
    ray_actor_options:
      num_cpus: 1
      num_gpus: 0.25
  - name: WhisperAPI
    max_outgoing_requests: 1
    autoscaling_config:
      target_outgoing_requests: 25
      min_replicas: 8
      max_replicas: 40
    ray_actor_options:
      num_cpus: 1


Starting load test...
Concurrency Avg Latency  P95 Latency  Error Rate
--------------------------------------------
4          0.55........ 0.00........ 0.0.......%
8          0.57........ 0.00........ 0.0.......%
16         0.63........ 0.00........ 0.0.......%
32         0.73........ 1.29........ 0.0.......%
64         1.03........ 1.78........ 0.0.......%
128        1.78........ 2.59........ 0.0.......%
192        2.49........ 4.02........ 0.0.......%
256        2.86........ 4.61........ 0.0.......%
512        4.98........ 8.45........ 0.0.......%

Starting load test...
Concurrency Avg Latency  P95 Latency  Error Rate
--------------------------------------------
4          0.49........ 0.00........ 0.0.......%
8          0.54........ 0.00........ 0.0.......%
16         0.59........ 0.00........ 0.0.......%
32         0.70........ 1.20........ 0.0.......%
64         1.08........ 1.86........ 0.0.......%
128        1.76........ 2.78........ 0.0.......%
192        2.28........ 3.54........ 0.0.......%
256        2.82........ 4.49........ 0.0.......%
512        4.86........ 8.60........ 0.0.......%

-------------------

"/home/marco/Development/mglabs/stt-server/audio/test_audio.wav"

curl -X POST \
  https://marcoeg--whisper-transcription-transcribe.modal.run \
  -F "audio_file=@/home/marco/Development/mglabs/stt-server/audio/test_audio.wav" \
  -F "model_size=base" \
  -F "language=en"

  (venv-3.9) marco@genai:~/Development/mglabs/stt-server/modal$ modal deploy whisper_app.py


---

Yes, the Ray cluster you described (1 `c5.2xlarge` head node and 2 `g5.2xlarge` worker nodes) can absolutely provide inference for an open-source large language model (LLM) alongside Whisper speech-to-text, using Rayâ€™s distributed computing capabilities. Hereâ€™s how and why itâ€™s feasible, along with some considerations:

---

### Why It Works
1. **Rayâ€™s Flexibility**:
   - Ray is designed for distributed task execution and supports both CPU- and GPU-based workloads. It can handle Whisper (speech-to-text) and LLM inference (text generation) simultaneously by distributing tasks across the cluster.
   - Rayâ€™s **actors** (stateful workers) and **tasks** (stateless functions) can manage model inference, allowing you to load Whisper on one set of resources and an LLM on another, or even share resources dynamically.

2. **Hardware Suitability**:
   - **Head Node (`c5.2xlarge`)**: 8 vCPUs, 16 GB RAM, no GPU. Suitable for coordination, preprocessing (e.g., audio/text tokenization), and lightweight tasks. It can run Rayâ€™s scheduler and serve as the entry point.
   - **Worker Nodes (2 `g5.2xlarge`)**: Each has 8 vCPUs, 32 GB RAM, and an NVIDIA A10G GPU (24 GB VRAM). The GPUs are critical for accelerating both Whisper (ASR) and LLM inference, as both benefit from GPU parallelism.
   - The A10Gâ€™s 24 GB VRAM can handle medium-sized LLMs (e.g., 7Bâ€“13B parameters) for inference, depending on quantization (e.g., 4-bit or 8-bit), alongside Whisper models (e.g., Whisper Large, ~1.5 GB).

3. **Workload Compatibility**:
   - **Whisper**: OpenAIâ€™s Whisper models (e.g., `large-v3`) are GPU-accelerated for real-time or batch transcription. With low traffic (100 hours/month), it uses minimal GPU time.
   - **LLM Inference**: Open-source LLMs like LLaMA, Mistral, or Falcon can run inference on GPUs. Ray can distribute requests across the `g5.2xlarge` nodes, leveraging their A10G GPUs.
   - Both workloads are inference-based (not training), which aligns with the clusterâ€™s compute and memory capacity.

4. **Ray Serve**:
   - Ray Serve, a scalable model-serving library, can deploy both Whisper and an LLM as separate endpoints (e.g., `/whisper` for ASR, `/llm` for text generation) on the same cluster. It handles routing, scaling, and resource allocation.

---

### Feasibility Assessment
- **Resource Sharing**:
  - Each `g5.2xlarge` has 24 GB GPU VRAM. Whisper Large (~1.5 GB) and a 7B-parameter LLM (e.g., ~5â€“7 GB with 4-bit quantization) can fit on one GPU simultaneously, leaving room for batch processing.
  - With 2 workers, you could dedicate one `g5.2xlarge` to Whisper and the other to the LLM, or share both models across both GPUs for load balancing.
- **Throughput**: 
  - Whisper: 100 hours/month (~3.3 hours/day) is low traffic, leaving ample GPU cycles.
  - LLM: Depends on request rate, but a 7B model on an A10G can handle ~10â€“20 tokens/second for inference, sufficient for low-to-moderate traffic (e.g., chatbot or text completion).
- **CPU/RAM**: The `c5.2xlarge` head node and `g5.2xlarge` workers have enough CPU/RAM (8 vCPUs, 16â€“32 GB) for preprocessing, tokenization, and Rayâ€™s overhead.

---

### Implementation with Ray
1. **Model Loading**:
   - Use Ray actors to instantiate Whisper and the LLM on the `g5.2xlarge` GPUs. Example:
     ```python
     import ray
     from transformers import WhisperProcessor, WhisperForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer

     @ray.remote(num_gpus=1)
     class WhisperActor:
         def __init__(self):
             self.processor = WhisperProcessor.from_pretrained("openai/whisper-large-v3")
             self.model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v3").to("cuda")

         def transcribe(self, audio):
             inputs = self.processor(audio, return_tensors="pt").to("cuda")
             return self.model.generate(**inputs)

     @ray.remote(num_gpus=1)
     class LLMActor:
         def __init__(self):
             self.tokenizer = AutoTokenizer.from_pretrained("mistralai/Mixtral-7B-v0.1")
             self.model = AutoModelForCausalLM.from_pretrained("mistralai/Mixtral-7B-v0.1", load_in_4bit=True).to("cuda")

         def generate(self, prompt):
             inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
             return self.model.generate(**inputs)
     ```
2. **Deployment with Ray Serve**:
   - Serve both models as endpoints:
     ```python
     from ray import serve

     serve.start()
     serve.deployment(WhisperActor).options(num_replicas=1).deploy()
     serve.deployment(LLMActor).options(num_replicas=1).deploy()
     ```
   - Access via HTTP or Python client, e.g., `curl http://<head-ip>:8000/whisper` or `ray.get(whisper_actor.transcribe.remote(audio))`.

3. **Resource Allocation**:
   - Ray automatically assigns tasks to available GPUs. You can pin actors to specific nodes or let Ray load-balance.

---

### Considerations
1. **VRAM Limits**:
   - A 13B LLM with 4-bit quantization (~10â€“12 GB) plus Whisper (~1.5 GB) might push the A10Gâ€™s 24 GB VRAM limit. Test your LLM size (e.g., stick to 7B models like Mistral-7B if co-running).
2. **Traffic**:
   - Low Whisper traffic leaves room, but high LLM traffic (e.g., thousands of requests/day) might require a third `g5.2xlarge` or request batching.
3. **Latency**:
   - Whisper and LLM inference on A10G GPUs are fast (e.g., <1s for short audio/text), but shared GPU usage might add minor delays. Profile with your workload.
4. **Cost**:
   - No additional hardware cost since the cluster supports both. The $2,043.64 (on-demand) or $1,359.02 (reserved) monthly estimate from my last report holds.

---

### Conclusion
Yes, your cluster can handle both Whisper and an open-source LLM for inference using Ray, leveraging the `g5.2xlarge` GPUs for acceleration. Start with a 7B-parameter LLM (e.g., Mistral-7B) to ensure VRAM compatibility, and use Ray Serve for deployment. If LLM traffic scales, consider adding a worker node or optimizing with quantization. Want help picking an LLM or setting this up? Let me know!


-----------

Local deployment
ðŸ““ How to Start Local Ray Serve for Whisper_serve
1. Start Ray cluster locally

ray start --head
This starts a local Ray cluster.

It prints something like:

Local node IP: 127.0.0.1
Dashboard: http://127.0.0.1:8265

2. Set your Ray dashboard address

export RAY_DASHBOARD_ADDRESS="http://127.0.0.1:8265"

3. Set your project root to the folder containing whisper_serve

cd /home/marco/Development/mglabs/stt-server
export PYTHONPATH="$(pwd)"
(âš¡ This makes sure Ray Serve finds whisper_serve as a Python package.)

4. Fix local imports to be relative (already done âœ…)
In whisper_serve/main.py, always use:

python
from .models import WhisperTranscriber
from .api import WhisperAPI
from .logger import logger

5. Deploy the server

serve deploy serve_config.local.yaml -a $RAY_DASHBOARD_ADDRESS

You will see a success message if deployment works.

Check with:

serve status -a $RAY_DASHBOARD_ADDRESS


Stop ray:
ray stop --force

----

source venv-3.9/bin/activate
pip install --upgrade pip
pip install ray[serve] ray[default] vllm torch transformers fastapi python-multipart accelerate
pip install openai-whisper

ðŸ› ï¸ How to Install pyenv (Ubuntu/Linux)
Install system dependencies

These are required so that pyenv can build different Python versions properly:

bash
Copy
Edit
sudo apt-get update
sudo apt-get install -y make build-essential libssl-dev zlib1g-dev \
libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \
libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev git
âœ… This ensures all necessary libraries (including bz2) are there.

Install pyenv itself

Use the official installer script:

bash
Copy
Edit
curl https://pyenv.run | bash
âœ… This installs pyenv, pyenv-virtualenv, and pyenv-update automatically.

Configure your shell

Add the following lines to your shell configuration file:

If you use bash:

bash
Copy
Edit
echo -e '\n# pyenv setup\nexport PATH="$HOME/.pyenv/bin:$PATH"\neval "$(pyenv init --path)"\neval "$(pyenv init -)"\neval "$(pyenv virtualenv-init -)"' >> ~/.bashrc
source ~/.bashrc
If you use zsh:

bash
Copy
Edit
echo -e '\n# pyenv setup\nexport PATH="$HOME/.pyenv/bin:$PATH"\neval "$(pyenv init --path)"\neval "$(pyenv init -)"\neval "$(pyenv virtualenv-init -)"' >> ~/.zshrc
source ~/.zshrc
âœ… This makes pyenv available automatically every time you open a terminal.

Verify pyenv is working

Close and reopen your terminal or do:

bash
Copy
Edit
exec "$SHELL"
Then run:

bash
Copy
Edit
pyenv --version
âœ… You should see something like:

nginx
Copy
Edit
pyenv 2.3.23
ðŸš€ How to install Python with pyenv
Once pyenv is ready:

bash
Copy
Edit
pyenv install 3.9.18
pyenv global 3.9.18
âœ… This will download, build, and set Python 3.9.18 as your default version.

âœ… bz2, lzma, and everything else will be properly built.

Check:

bash
Copy
Edit
python --version
# Python 3.9.18

python -c "import bz2; print('bz2 available âœ…')"
# bz2 available âœ…


---

Step 1: Force pyenv Python
Manually:

bash
Copy
Edit
pyenv shell 3.9.18
or even stronger:

bash
Copy
Edit
pyenv local 3.9.18
This forces the shell to use pyenv's Python 3.9.18.

âœ… Check again:

bash
Copy
Edit
which python
python --version
python -c "import bz2; print('bz2 available âœ…')"
Step 2: Recreate venv with good Python
VERY IMPORTANT:
âš¡ You must delete the broken venv and rebuild it from clean pyenv Python.

From your project root:

bash
Copy
Edit
rm -rf venv-3.9
python -m venv venv-3.9
source venv-3.9/bin/activate
pip install --upgrade pip
pip install ray[serve] ray[default] torch transformers fastapi python-multipart accelerate vllm openai-whisper
âœ… Now python inside venv should be the pyenv's 3.9.18 version.

âœ… import bz2 will succeed.


python3.9 -m venv venv-3.9
source venv-3.9/bin/activate
pip install --upgrade pip
pip install "ray[serve]"==2.9.3
pip install torch transformers fastapi python-multipart vllm==0.4.0 openai-whisper


python -c "import ray; print(ray.__version__)"

--
 curl  -X POST http://127.0.0.1:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Write a short motivational quote."}'


---
  sampling_params = SamplingParams(
      temperature=0.1,
      top_p=0.9,
      max_tokens=200,  # Add a limit
      repetition_penalty=1.1,  # Penalize repeated outputs
  )

curl -X POST http://127.0.0.1:8000/transcribe   -F "audio_file=@/home/marco/Documents/PromptBet/downloads/0d19cb7a-1b12-45ea-b1ac-fbbb06f986a7_stream.wav"
curl -X POST http://127.0.0.1:8000/transcribe   -F "audio_file=@/home/marco/Documents/PromptBet/downloads/2a17f9fd-da2b-4ce0-a4ae-47d1d5faf6a8_stream.wav"
curl -X POST http://127.0.0.1:8000/transcribe   -F "audio_file=@/home/marco/Documents/PromptBet/downloads/2c282e43-acdc-4da3-8bc9-795074fbe027_stream.wav"
curl -X POST http://127.0.0.1:8000/transcribe   -F "audio_file=@/home/marco/Documents/PromptBet/downloads/2f74c965-4e35-401f-9ec3-137b58287777_stream.wav"
curl -X POST http://127.0.0.1:8000/transcribe   -F "audio_file=@/home/marco/Documents/PromptBet/downloads/3db4dbb1-115d-4052-8293-1616df01f0b5_stream.wav" curl -X POST http://127.0.0.1:8000/transcribe   -F "audio_file=@/home/marco/Documents/PromptBet/downloads/0d19cb7a-1b12-45ea-b1ac-fbbb06f986a7_stream.wav"

curl -X POST http://127.0.0.1:8000/generate   -H "Content-Type: application/json"   -d '{"prompt": "Write a short motivational quote."}'
curl -X POST http://127.0.0.1:8000/generate   -H "Content-Type: application/json"   -d '{"prompt": "Why the sky is blue?"}'