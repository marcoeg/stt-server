runtime_env:
  working_dir: .
  env_vars:
    RAY_SERVE_REQUEST_PROCESSING_TIMEOUT_S: "3600"
    RAY_SERVE_HTTP_PROXY_TIMEOUT_S: "3600"
    #PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"   # <<< DO NOT FORGET THIS!!!
    PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True,max_split_size_mb:128" 

applications:
  - name: default
    route_prefix: /
    import_path: whisper_serve.main:root_app
    args:
      whisper_model_args:
        model_size: base
      llama_model_args:
        model_path: shuyuej/Llama-3.2-1B-Instruct-GPTQ
    deployments:
      - name: WhisperTranscriber
        num_replicas: 1
        ray_actor_options:
          num_cpus: 2
          num_gpus: 0.5
      - name: Llama3Inference
        num_replicas: 1
        ray_actor_options:
          num_cpus: 2
          num_gpus: 0.5
      - name: RootApp
        num_replicas: 1
        ray_actor_options:
          num_cpus: 1
