
---------------------------------------------------------------

---

# Start the cluster in AWS:
ray up cluster.yaml --no-config-cache 

# After the head node is setup -- 

#  To monitor autoscaling:
ray exec /home/marco/Development/mglabs/stt-server/cluster.yaml 'tail -n 100 -f /tmp/ray/session_latest/logs/monitor*'

export RAY_HEAD_ADDRESS=$(ray get-head-ip cluster.yaml | tail -n 1); echo $RAY_HEAD_ADDRESS 
export RAY_DASHBOARD_ADDRESS="http://$RAY_HEAD_ADDRESS:8265"; echo $RAY_DASHBOARD_ADDRESS

# Access the dashboard in a browser at http://$RAY_HEAD_ADDRESS:8265

# After the workers are setup --

# Set up the workload for remote deployment:
zip -r whisper_serve.zip whisper_serve/
aws s3 cp whisper_serve.zip s3://ntoplabs-0001

# Start the worload:
serve deploy serve_config.yaml -a $RAY_DASHBOARD_ADDRESS
serve status -a $RAY_DASHBOARD_ADDRESS

# Test endpoint:
curl -X POST http://$RAY_HEAD_ADDRESS:8000/transcribe \
  -F "audio_file=@./audio/test_audio.wav"

# Terminate the cluster
ray down cluster.yaml 

# Log into the head node
ray attach cluster.yaml

-- LOCAL -------------------------------------------------------------

applications:
- name: whisper_service
  route_prefix: /
  import_path: main:app
---

# Start a local cluster:
ray start --head

export RAY_DASHBOARD_ADDRESS="http://127.0.0.1:8265"
# suggest but possibly not really needed
export PYTHONPATH="/home/marco/Development/mglabs/stt-server"

# entry point main.py
serve deploy serve_config_local.yaml -a $RAY_DASHBOARD_ADDRESS 
 
serve status -a $RAY_DASHBOARD_ADDRESS

curl -X POST http://localhost:8000/transcribe \
  -F "audio_file=@./audio/test_audio.wav"

-------------------------------------------------------------
# another working setup using transformers and entry point main-temp1.py

config file: serve_config.local.yaml:
applications:
- name: whisper_service
  route_prefix: /
  import_path: main-temp1:app
...
...
--

export PYTHONPATH="/home/marco/Development/mglabs/stt-server"
serve deploy serve_config.local.yaml -a $RAY_DASHBOARD_ADDRESS 
serve status -a $RAY_DASHBOARD_ADDRESS

serve shutdown -a $RAY_DASHBOARD_ADDRESS
-------------------------------------------------------------

# Watch number of concurrent actors and memory
watch -n 1 'nvidia-smi | grep "WhisperTranscriber" | wc -l'
nvidia-smi -l 1

# Monitor setup (after head node started)
ray exec cluster.yaml 'tail -n 100 -f /tmp/ray/session_latest/logs/monitor*'

-------------------------------------------------------------


# Get head node IP
ray get-head-ip cluster.yaml
export RAY_HEAD_ADDRESS=$(ray get-head-ip cluster.yaml | tail -n 1); echo $RAY_HEAD_ADDRESS

# Check cluster status
ray status --address=$RAY_HEAD_ADDRESS:6379

# Test service
curl -X POST http://$RAY_HEAD_ADDRESS:8000/transcribe \
  -F "audio_file=@./audio/test_audio.wav"

# Check Ray dashboard
curl http://$RAY_HEAD_ADDRESS:8265

# Log into the head node (ssh)
ray attach cluster.yaml

# Find workers IP
aws ec2 describe-instances --filters "Name=tag:ray-node-type,Values=worker" "Name=tag:ray-cluster-name,Values=whisper-serve-cluster" --query 'Reservations[*].Instances[*].PublicIpAddress' --output text


# Sequence of checks to perform after the head node is started:

# Check Ray Dashboard accessibility:
curl http://<head-node-ip>:8265/api/overview

Check Ray Client port:
nc -vz <head-node-ip> 10001

# Check GCS port:
nc -vz <head-node-ip> 6379

# Check Node logs:
```bash
ray exec cluster.yaml 'tail -n 50 /tmp/ray/session_*/logs/*'

# Check Ray status:
ray status cluster.yaml

# Verify environment variables:
ray exec cluster.yaml 'env | grep RAY_'

# Check resource availability:
ray exec cluster.yaml 'ray status --address=localhost:6379 | grep "resources"'
ray exec custer.yaml 'env | grep RAY_'



----------------------------------------------
----------------------------------------------

Starting load test...
Concurrency Avg Latency  P95 Latency  Error Rate
--------------------------------------------
4          0.49........ 0.00........ 0.0.......%
8          0.69........ 0.00........ 0.0.......%
16         1.28........ 0.00........ 0.0.......%
32         2.38........ 4.18........ 0.0.......%
64         4.52........ 8.41........ 0.0.......%
128        9.02........ 17.41....... 0.0.......%
192        14.18....... 25.94....... 0.0.......%
256        18.55....... 36.21....... 0.0.......%
512        37.41....... 71.16....... 0.0.......%


applications:
- name: whisper_service
  route_prefix: /
  import_path: main:app
  runtime_env:
    pip:
      - torch
      - transformers
      - fastapi
      - python-multipart
  deployments:
  - name: WhisperTranscriber
    max_outgoing_requests: 5
    autoscaling_config:
      target_outgoing_requests: 2
      min_replicas: 1
      max_replicas: 50
    ray_actor_options:
      num_cpus: 1
      num_gpus: 0.1
  - name: WhisperAPI
    max_outgoing_requests: 5
    autoscaling_config:
      target_outgoing_requests: 2
      min_replicas: 1
      max_replicas: 50
    ray_actor_options:
      num_cpus: 1


----------------------------------------------
----------------------------------------------

Starting load test...
Concurrency Avg Latency  P95 Latency  Error Rate
--------------------------------------------
4          0.39........ 0.00........ 0.0.......%
8          0.62........ 0.00........ 0.0.......%
16         1.09........ 0.00........ 0.0.......%
32         2.03........ 3.60........ 0.0.......%
64         3.92........ 7.36........ 0.0.......%
128        7.28........ 12.68....... 0.0.......%
192        7.49........ 14.35....... 0.0.......%
256        10.23....... 18.67....... 0.0.......%
512        19.80....... 37.21....... 0.0.......%


applications:
- name: whisper_service
  route_prefix: /
  import_path: main:app
  runtime_env:
    pip:
      - torch
      - transformers
      - fastapi
      - python-multipart
  deployments:
  - name: WhisperTranscriber
    max_outgoing_requests: 10  # Increased from 5
    autoscaling_config:
      target_outgoing_requests: 4  # Increased from 2
      min_replicas: 2  # Increased from 1
      max_replicas: 10  # Reduced from 50 to be more realistic with GPU memory
      upscale_delay_s: 10  # Added to make scaling more responsive
      downscale_delay_s: 30  # Added to prevent rapid scale-down
    ray_actor_options:
      num_cpus: 1
      num_gpus: 0.2  # Increased from 0.1 to allow fewer but more powerful actors
      memory: 4000_000_000  # 4GB memory per actor
  - name: WhisperAPI
    max_outgoing_requests: 15  # Increased from 5
    autoscaling_config:
      target_outgoing_requests: 5  # Increased from 2
      min_replicas: 2  # Increased from 1
      max_replicas: 20  # Adjusted based on CPU availability
    ray_actor_options:
      num_cpus: 1
      memory: 2000_000_000  # 2GB memory per actor


----------------------------------------------
----------------------------------------------

Starting load test...
Concurrency Avg Latency  P95 Latency  Error Rate
--------------------------------------------
4          0.50........ 0.00........ 0.0.......%
8          0.53........ 0.00........ 0.0.......%
16         0.81........ 0.00........ 0.0.......%
32         1.76........ 2.29........ 0.0.......%
64         3.01........ 4.67........ 0.0.......%
128        5.40........ 8.95........ 0.0.......%
192        7.93........ 13.19....... 0.0.......%
256        9.61........ 17.11....... 0.0.......%
512        18.44....... 33.66....... 0.0.......%
proxy_location: EveryNode

http_options:
  host: 0.0.0.0
  port: 8000

applications:
- name: whisper_service
  route_prefix: /
  import_path: main:app
  runtime_env:
    pip:
      - torch
      - transformers
      - fastapi
      - python-multipart
  deployments:
  - name: WhisperTranscriber
    max_outgoing_requests: 25  # Increased for more concurrent processing
    autoscaling_config:
      target_outgoing_requests: 10
      min_replicas: 8  # Increased based on memory availability
      max_replicas: 25  # (16376MiB - 95MiB Xorg) / 592MiB â‰ˆ 27 possible actors
      upscale_delay_s: 10
      downscale_delay_s: 30
    ray_actor_options:
      num_cpus: 1
      num_gpus: 0.037  # 1/27 of GPU (allows maximum concurrent actors)
      memory: 2000_000_000  # Adjusted based on observed usage
      runtime_env:
        env_vars:
          CUDA_VISIBLE_DEVICES: "0"
  - name: WhisperAPI
    max_outgoing_requests: 30
    autoscaling_config:
      target_outgoing_requests: 12
      min_replicas: 8
      max_replicas: 30
    ray_actor_options:
      num_cpus: 1
      memory: 2000_000_000


----------------------------------------------
----------------------------------------------


Concurrency Avg Latency  P95 Latency  Error Rate
--------------------------------------------
4          0.50........ 0.00........ 0.0.......%
8          0.52........ 0.00........ 0.0.......%
16         0.84........ 0.00........ 0.0.......%
32         1.76........ 2.33........ 0.0.......%
64         2.99........ 4.54........ 0.0.......%
128        5.37........ 8.92........ 0.0.......%
192        7.60........ 12.55....... 0.0.......%
256        9.52........ 16.80....... 0.0.......%
512        17.89....... 33.23....... 0.0.......%

proxy_location: EveryNode

http_options:
  host: 0.0.0.0
  port: 8000
  
applications:
- name: whisper_service
  route_prefix: /
  import_path: main:app
  runtime_env:
    pip:
      - torch
      - transformers
      - fastapi
      - python-multipart
  deployments:
  - name: WhisperTranscriber
    max_outgoing_requests: 25
    autoscaling_config:
      target_outgoing_requests: 10
      min_replicas: 8
      max_replicas: 25
      upscale_delay_s: 10
      downscale_delay_s: 30
      upscale_batch_size: 2  # Only addition: control scaling batch size
    ray_actor_options:
      num_cpus: 1
      num_gpus: 0.037
      memory: 2000_000_000
      runtime_env:
        env_vars:
          CUDA_VISIBLE_DEVICES: "0"
  - name: WhisperAPI
    max_outgoing_requests: 30
    autoscaling_config:
      target_outgoing_requests: 12
      min_replicas: 8
      max_replicas: 30
    ray_actor_options:
      num_cpus: 1
      memory: 2000_000_000

WARNING 2024-11-24 13:29:46,412 whisper_service_WhisperTranscriber zyx1jvph replica.py:571 - Replica at capacity of max_ongoing_requests=5, rejecting request 22370ef1-eddb-4c93-a788-324012c9f83f.
